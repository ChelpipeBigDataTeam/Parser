{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading https://files.pythonhosted.org/packages/db/9c/cb15b2dc6003a805afd21b9b396e0e965800765b51da72fe17cf340b9be2/Scrapy-1.5.0-py2.py3-none-any.whl (251kB)\n",
      "\u001b[K    100% |████████████████████████████████| 256kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting service-identity (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/fa/995e364220979e577e7ca232440961db0bf996b6edaf586a7d1bd14d81f1/service_identity-17.0.0-py2.py3-none-any.whl\n",
      "Collecting queuelib (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyOpenSSL in /srv/conda/lib/python3.6/site-packages (from scrapy)\n",
      "Collecting Twisted>=13.1.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/2a/e9e4fb2e6b2f7a75577e0614926819a472934b0b85f205ba5d5d2add54d0/Twisted-18.4.0.tar.bz2 (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 213kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
      "Requirement already satisfied: six>=1.5.2 in /srv/conda/lib/python3.6/site-packages (from scrapy)\n",
      "Collecting cssselect>=0.9 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/37/94/40c93ad0cadac0f8cb729e1668823c71532fd4a7361b141aec535acb68e3/w3lib-1.19.0-py2.py3-none-any.whl\n",
      "Collecting lxml (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/81/5a3e70c8adc20fb295a2f4c9fdf09af8295c7a00ccec6ee3d31084cbf272/lxml-4.2.3-cp36-cp36m-manylinux1_x86_64.whl (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 109kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting parsel>=1.1 (from scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/b4/2fd37d6f6a7e35cbc4c2613a789221ef1109708d5d4fb9fd5f6f721a43c9/parsel-1.4.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules (from service-identity->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/19/02/fa63f7ba30a0d7b925ca29d034510fc1ffde53264b71b4155022ddf3ab5d/pyasn1_modules-0.2.2-py2.py3-none-any.whl (62kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 6.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs (from service-identity->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/59/cedf87e91ed541be7957c501a92102f9cc6363c623a7666d69d51c78ac5b/attrs-18.1.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1 (from service-identity->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/70/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=1.9 in /srv/conda/lib/python3.6/site-packages (from pyOpenSSL->scrapy)\n",
      "Collecting zope.interface>=4.4.2 (from Twisted>=13.1.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/8a/657532df378c2cd2a1fe6b12be3b4097521570769d4852ec02c24bd3594e/zope.interface-4.5.0.tar.gz (151kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting constantly>=15.1 (from Twisted>=13.1.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
      "Collecting incremental>=16.10.1 (from Twisted>=13.1.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
      "Collecting Automat>=0.3.0 (from Twisted>=13.1.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=13.1.0->scrapy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/b6/84d0c863ff81e8e7de87cff3bd8fd8f1054c227ce09af1b679a8b17a9274/hyperlink-18.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna>=2.1 in /srv/conda/lib/python3.6/site-packages (from cryptography>=1.9->pyOpenSSL->scrapy)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /srv/conda/lib/python3.6/site-packages (from cryptography>=1.9->pyOpenSSL->scrapy)\n",
      "Requirement already satisfied: cffi>=1.7 in /srv/conda/lib/python3.6/site-packages (from cryptography>=1.9->pyOpenSSL->scrapy)\n",
      "Requirement already satisfied: setuptools in /srv/conda/lib/python3.6/site-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->scrapy)\n",
      "Requirement already satisfied: pycparser in /srv/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=1.9->pyOpenSSL->scrapy)\n",
      "Building wheels for collected packages: Twisted, PyDispatcher, zope.interface\n",
      "  Running setup.py bdist_wheel for Twisted ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/b3/76/f7/85353c829c0881e74b5366ce0ed59042b098bb4903e2da8828\n",
      "  Running setup.py bdist_wheel for PyDispatcher ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
      "  Running setup.py bdist_wheel for zope.interface ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/c6/b2/d2/be6785a207eaa58d76debc10c9d5c66196b40a88abb61d6af7\n",
      "Successfully built Twisted PyDispatcher zope.interface\n",
      "Installing collected packages: pyasn1, pyasn1-modules, attrs, service-identity, queuelib, zope.interface, constantly, incremental, Automat, hyperlink, Twisted, PyDispatcher, cssselect, w3lib, lxml, parsel, scrapy\n",
      "Successfully installed Automat-0.7.0 PyDispatcher-2.0.5 Twisted-18.4.0 attrs-18.1.0 constantly-15.1.0 cssselect-1.0.3 hyperlink-18.0.0 incremental-17.5.0 lxml-4.2.3 parsel-1.4.0 pyasn1-0.4.3 pyasn1-modules-0.2.2 queuelib-1.5.0 scrapy-1.5.0 service-identity-17.0.0 w3lib-1.19.0 zope.interface-4.5.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoap4\n",
      "\u001b[31m  Could not find a version that satisfies the requirement beautifulsoap4 (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for beautifulsoap4\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoap4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: bs4\n",
      "  Running setup.py bdist_wheel for bs4 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "Installing collected packages: beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.6.0 bs4-0.0.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /srv/conda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-02 04:00:47 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n",
      "2018-07-02 04:00:47 [scrapy.utils.log] INFO: Versions: lxml 4.2.3.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.6.4 | packaged by conda-forge | (default, Dec 23 2017, 16:31:06) - [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)], pyOpenSSL 17.2.0 (OpenSSL 1.0.2n  7 Dec 2017), cryptography 2.0.3, Platform Linux-4.14.22+-x86_64-with-debian-stretch-sid\n",
      "2018-07-02 04:00:47 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2018-07-02 04:00:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2018-07-02 04:00:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2018-07-02 04:00:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-07-02 04:00:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-07-02 04:00:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-07-02 04:00:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-07-02 04:00:47 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2018-07-02 04:00:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=1&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=> (referer: None)\n",
      "2018-07-02 04:00:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=3&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=> (referer: https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=1&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=)\n",
      "2018-07-02 04:00:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=4&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=> (referer: https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=3&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=)\n",
      "2018-07-02 04:00:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=5&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=> (referer: https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=4&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=)\n",
      "2018-07-02 04:00:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=6&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=> (referer: https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=5&date_period=all&city=&search=&rec_participation=&reg_open=Y&international=)\n",
      "2018-07-02 04:00:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2018-07-02 04:00:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2686,\n",
      " 'downloader/request_count': 5,\n",
      " 'downloader/request_method_count/GET': 5,\n",
      " 'downloader/response_bytes': 14197,\n",
      " 'downloader/response_count': 5,\n",
      " 'downloader/response_status_count/200': 5,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2018, 7, 2, 4, 0, 55, 246436),\n",
      " 'log_count/DEBUG': 6,\n",
      " 'log_count/INFO': 7,\n",
      " 'memusage/max': 79790080,\n",
      " 'memusage/startup': 79790080,\n",
      " 'request_depth_max': 4,\n",
      " 'response_received_count': 5,\n",
      " 'scheduler/dequeued': 5,\n",
      " 'scheduler/dequeued/memory': 5,\n",
      " 'scheduler/enqueued': 5,\n",
      " 'scheduler/enqueued/memory': 5,\n",
      " 'start_time': datetime.datetime(2018, 7, 2, 4, 0, 47, 877882)}\n",
      "2018-07-02 04:00:55 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "class SpidyQuotesSpider(scrapy.Spider):\n",
    "    name = 'spidyquotes'\n",
    "    \n",
    "    quotes_base_url = 'https://www.exportcenter.ru/events/?AJAX_MODE=1&PAGEN_1=%s&date_period=all&city=&search=&rec_participation=&reg_open=Y&international='\n",
    "    start_urls = [quotes_base_url % 1]\n",
    "    download_delay = 1.5\n",
    "\n",
    "    def parse(self, response):\n",
    "        data = []\n",
    "        if (os.path.exists(\"log.txt\")):\n",
    "            with open(\"log.txt\") as f:\n",
    "                for line in f:\n",
    "                    tmp = line\n",
    "                    data.append(tmp)\n",
    "        \n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        news = soup.find_all('div', class_='event-block')\n",
    "        \n",
    "        for new in news:\n",
    "            url = 'https://www.exportcenter.ru' + new.find('div', class_='event-item__desc').find('a').get('data-url')\n",
    "            name = new.find('a', class_ = 'event-item__title js-events-detail-link').getText().replace('  ','').replace('\\n','').replace('\\r','')\n",
    "            date = new.find('span', class_ = 'event-item__date_day').getText() + '/' + new.find('span', class_ = 'event-item__date_month').getText()\n",
    "            \n",
    "            is_ = False\n",
    "            for i in range(len(data)):\n",
    "                if name in data[i]:\n",
    "                    is_ = True\n",
    "                    break\n",
    "            \n",
    "            if is_ == False: \n",
    "                with open('log.txt', 'a') as f:\n",
    "                    f.write('name: {0}, url: {1}, date: {2}\\n'.format(name, url, date))\n",
    "                yield {\n",
    "                    'name': name,\n",
    "                    'url': url,\n",
    "                    'date': date,\n",
    "                }\n",
    "            \n",
    "        if soup.find('div', class_='event-btn_more js-events-go-next'):\n",
    "            next_page = int(soup.find('div', class_='event-btn_more js-events-go-next').get('data-next-page'))\n",
    "            yield scrapy.Request(self.quotes_base_url % next_page)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess()\n",
    "\n",
    "    process.crawl(SpidyQuotesSpider)\n",
    "    process.start() # the script will block here until the crawling is finished\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
